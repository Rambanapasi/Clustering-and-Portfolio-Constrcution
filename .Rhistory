funds_considered %>% filter(Tickers %in% bm) %>%
tbl2xts::tbl_xts(cols_to_xts = Ret, Colnames_Exact = T)
library(PerformanceAnalytics)
Moms <-
bind_rows(data.frame(PerformanceAnalytics::Return.annualized.excess(Fundxts, BMxts) ) %>% round(., 3),
data.frame(PerformanceAnalytics::AdjustedSharpeRatio(Fundxts) ) %>% round(., 3),
data.frame(AverageDrawdown(Fundxts, scale = 12)) %>% round(., 3),
data.frame(TrackingError(Ra = Fundxts, Rb = BMxts, scale = 12)) %>% round(., 3),
data.frame(PerformanceAnalytics::CAPM.beta(Ra = Fundxts, Rb = BMxts, Rf = 0)) %>% round(., 3),
data.frame(PerformanceAnalytics::CAPM.beta.bull(Ra = Fundxts, Rb = BMxts, Rf = 0)) %>% round(., 3),
data.frame(PerformanceAnalytics::CAPM.beta.bear(Ra = Fundxts, Rb = BMxts, Rf = 0)) %>% round(., 3)
# # data.frame(PerformanceAnalytics::UpDownRatios(Ra = Fundxts, Rb = BMxts, method = "Percent", side = "Up")) %>% round(., 3),
# data.frame(PerformanceAnalytics::CVaR(R = Fundxts, p = 0.05, method = "modified")) %>% round(., 3)
) %>%  tibble::rownames_to_column("Info") %>%
mutate(Period = glue::glue("Last {Months_LookBack} Months"), Info = c("Returns Excess (Ann.)", "Adj. Sharpe Ratio", "Avg DD", "Tracking Error", "Beta", "Beta Bull", "Beta Bear")) %>%
relocate(Period, .before = Info) %>% as_tibble()
colnames(Moms) <- gsub("comp_rtn_", "Portfolio ", colnames(Moms))
Moms
}
# create the group statistics
Tab_stats <-bind_rows(Moments_Comp(all_funds, 3), Moments_Comp(all_funds, 6), Moments_Comp(all_funds, 12)) %>%group_split(Period)
for (i in seq_along(Tab_stats)) {
saveRDS(Tab_stats[[i]], file = paste0("data/Period_", i, ".rds"))
}
# to avoid null datasets, filter out 2024
clusteringdata <- clusteringdata %>%
filter(date <= ymd(20231231) & !stock %in% volatile_stock)
start_date <- clusteringdata %>%
filter(date == first(date)) %>%
select(date) %>%
distinct() %>%
pull()
# get the end dates which will mark the end of our investment horizon
end_dates <- clusteringdata %>%
mutate(Y = format(date, "%Y")) %>%
group_by(Y) %>%
filter(date == last(date)) %>%
ungroup() %>%
select(date) %>%
distinct() %>%
pull()
# get different investment horizons by creating multiple datasets
investment_horizon_dfs <- list()
for (end_date in unique(end_dates)) {
df <- cluster.merging %>%
filter(date >= start_date & date <= end_date) %>%
arrange(date) %>%
select(-Company, -Sector, -YM) %>%
group_by(stock) %>%
mutate(rtn = px/lag(px) - 1) %>%
slice(-1)
# Use the current end_date in the list assignment
investment_horizon_dfs[[as.character(end_date)]] <- df
}
# from our ranking.df
# Extract stock names
names <- ranking.df %>% select(stock)
# Extract and prepare data for clustering
cluster_data <- ranking.df %>% ungroup() %>% select(-stock)
rownames(cluster_data) <- as.character(names$stock)
# Determine the optimal number of clusters using silhouette method
a <- fviz_nbclust(cluster_data, kmeans, method = "silhouette")
silhouette <- a$data %>% tibble()
silhouette$clusters <- as.numeric(silhouette$clusters)
value <- silhouette %>% filter(y == max(y)) %>% select(clusters) %>% pull()
# Perform k-means clustering
km_res <- kmeans(cluster_data, value, nstart = 1000, algorithm = "Lloyd")
clusters.names <- data.frame(
stock = names$stock,
clusters = km_res$cluster )
# portfolio list
# Merge corresponding clusters with investment_horizon_dfs
cluster_portfolio <- lapply(1:length(investment_horizon_dfs), function(i) {
# Merge based on the "stock" column
merged_df <- merge(investment_horizon_dfs[[i]], clusters.names, by = "stock")
})
# now see performance given our capping methodology
# at the rebalancing date you want to take the top n of each clusters and reweight.
# rolling backtest
rolling_Backtest <- list()
for (i in seq_along(investment_horizon_dfs)) {
investment_horizon <- investment_horizon_dfs[[i]]  # Use the loop variable to access different investment horizons
df <- investment_horizon
Rebalance_Days <- df %>%
mutate(Year = format(date, "%Y"), Month = format(date, "%b"), YM = format(date, "%y %b")) %>%
filter(Month %in% c("Mar", "Jun", "Sep", "Dec")) %>%
select(date, Year,  Month, YM ) %>% unique() %>%
group_by(YM) %>%
filter(date == last(date)) %>%
filter( date == last(date)) %>% distinct(date) %>%
pull(date)
# now for getting the top 3 stock in each cluster that way we have a a single capping objective, and to make this work, I assume that I create a volume weighted index, that at each rebalancing date the index is capped.
rebalance_col <-
df %>%
filter(date %in% Rebalance_Days) %>%
# Now we have to distinguish rebalances - to create something to group by:
mutate(RebalanceTime = format(date, "%Y%B")) %>%
# Now we can group...
group_by(RebalanceTime, cluster) %>% summarise(date = date, stock,  weight = vol/sum(vol) ) %>%
filter(!is.na(weight)) %>%
# Now trim down to 30 stocks and reweight so sum(w)=1
arrange(desc(weight)) %>%
top_n(3, weight) %>%
mutate(weight = weight/sum(weight)) %>%
ungroup() %>%
arrange(date)
# get unique stock to filter in the main dataframe
uniquestock <- rebalance_col %>% select(stock) %>% distinct() %>% pull()
cappedmonthly <-  monthly_data %>% filter(stock %in% uniquestock & date %in% Rebalance_Days & !stock %in% volatile_stock)
# now lets cap each at roughly equal weighting which is 35%
Cap <- rebalance_col %>%
# Split our df into groups (where the groups here are the rebalance dates:
group_split(RebalanceTime, cluster) %>%
# Apply the function Proportional_Cap_Foo to each rebalancing date:
map_df(~Proportional_Cap_Foo(., W_Cap = 0.35) ) %>%
select(-RebalanceTime)
static_backtest_performance <- left_join(Cap,cappedmonthly , c("stock", "date", "cluster"))
#
cluster_return_df <- static_backtest_performance %>%
group_by(date, cluster) %>%
mutate(weighted_return = weight * rtn) %>%
select(cluster, date, weighted_return) %>%
group_by(date, cluster) %>%
summarize( ret = sum(weighted_return)) %>% ungroup() %>% mutate(cluster = paste0("Cluster_", cluster))
rolling_Backtest[[i]] <- cluster_return_df
}
Moments <- bind_rows(data.frame(PerformanceAnalytics::Return.annualized.excess(df, bm, scale =4)) %>% round(., 3),
data.frame(PerformanceAnalytics::TrackingError(Ra = df, Rb = bm, scale =4 ))%>% round(., 3),
data.frame(PerformanceAnalytics::AdjustedSharpeRatio(df))%>% round(., 3),
data.frame(PerformanceAnalytics::AverageLength(df))%>% round(., 0),
data.frame( PerformanceAnalytics::maxDrawdown(df)%>% round(., 3))) %>%
tibble::rownames_to_column("Metrics") %>%
mutate("Investment Horizon" = glue::glue("{i} Year"), Info = c("Ann Excess Return", "Ann Tracking Error", "Adj. Sharpe Ratio", "DD Length", "Max DD")) %>%
relocate("Investment Horizon" , .before = Metrics) %>%  relocate("Info" , .before = Metrics) %>% as_tibble() %>% select(-Metrics)
df <- tbl2xts::tbl_xts(rolling_backtest, ret, cluster, Colnames_Exact = TRUE)
# now see performance given our capping methodology
# at the rebalancing date you want to take the top n of each clusters and reweight.
# rolling backtest
rolling_Backtest <- list()
for (i in seq_along(investment_horizon_dfs)) {
investment_horizon <- investment_horizon_dfs[[i]]  # Use the loop variable to access different investment horizons
df <- investment_horizon
Rebalance_Days <- df %>%
mutate(Year = format(date, "%Y"), Month = format(date, "%b"), YM = format(date, "%y %b")) %>%
filter(Month %in% c("Mar", "Jun", "Sep", "Dec")) %>%
select(date, Year,  Month, YM ) %>% unique() %>%
group_by(YM) %>%
filter(date == last(date)) %>%
filter( date == last(date)) %>% distinct(date) %>%
pull(date)
# now for getting the top 3 stock in each cluster that way we have a a single capping objective, and to make this work, I assume that I create a volume weighted index, that at each rebalancing date the index is capped.
rebalance_col <-
df %>%
filter(date %in% Rebalance_Days) %>%
# Now we have to distinguish rebalances - to create something to group by:
mutate(RebalanceTime = format(date, "%Y%B")) %>%
# Now we can group...
group_by(RebalanceTime, cluster) %>% summarise(date = date, stock,  weight = vol/sum(vol) ) %>%
filter(!is.na(weight)) %>%
# Now trim down to 30 stocks and reweight so sum(w)=1
arrange(desc(weight)) %>%
top_n(3, weight) %>%
mutate(weight = weight/sum(weight)) %>%
ungroup() %>%
arrange(date)
# get unique stock to filter in the main dataframe
uniquestock <- rebalance_col %>% select(stock) %>% distinct() %>% pull()
cappedmonthly <-  monthly_data %>% filter(stock %in% uniquestock & date %in% Rebalance_Days & !stock %in% volatile_stock)
# now lets cap each at roughly equal weighting which is 35%
Cap <- rebalance_col %>%
# Split our df into groups (where the groups here are the rebalance dates:
group_split(RebalanceTime, cluster) %>%
# Apply the function Proportional_Cap_Foo to each rebalancing date:
map_df(~Proportional_Cap_Foo(., W_Cap = 0.35) ) %>%
select(-RebalanceTime)
static_backtest_performance <- left_join(Cap,cappedmonthly , c("stock", "date", "cluster"))
#
cluster_return_df <- static_backtest_performance %>%
group_by(date, cluster) %>%
mutate(weighted_return = weight * rtn) %>%
select(cluster, date, weighted_return) %>%
group_by(date, cluster) %>%
summarize( ret = sum(weighted_return)) %>% ungroup() %>% mutate(cluster = paste0("Cluster_", cluster))
rolling_Backtest[[i]] <- cluster_return_df
}
# from the results, lets get some perfromance metrics
# clean our BM dataset
BM <- BM %>%
mutate(Year = format(date, "%Y"), Month = format(date, "%b"), YM = format(date, "%y %b")) %>%
filter(Month %in% c("Mar", "Jun", "Sep", "Dec")) %>%
select(date, BM)
rolling_results <- list()
for (i in seq_along(rolling_Backtest)) {
rolling_backtest <- rolling_Backtest[[i]]
df <- tbl2xts::tbl_xts(rolling_backtest, ret, cluster, Colnames_Exact = TRUE)
bm <- BM %>% filter(date >= ymd(20140331)) %>%  tbl2xts::tbl_xts(., Colnames_Exact = TRUE)
df[is.na(df)] <- 0
bm[is.na(bm)] <- 0
Moments <- bind_rows(data.frame(PerformanceAnalytics::Return.annualized.excess(df, bm, scale =4)) %>% round(., 3),
data.frame(PerformanceAnalytics::TrackingError(Ra = df, Rb = bm, scale =4 ))%>% round(., 3),
data.frame(PerformanceAnalytics::AdjustedSharpeRatio(df))%>% round(., 3),
data.frame(PerformanceAnalytics::AverageLength(df))%>% round(., 0),
data.frame( PerformanceAnalytics::maxDrawdown(df)%>% round(., 3))) %>%
tibble::rownames_to_column("Metrics") %>%
mutate("Investment Horizon" = glue::glue("{i} Year"), Info = c("Ann Excess Return", "Ann Tracking Error", "Adj. Sharpe Ratio", "DD Length", "Max DD")) %>%
relocate("Investment Horizon" , .before = Metrics) %>%  relocate("Info" , .before = Metrics) %>% as_tibble() %>% select(-Metrics)
Make_perc <-
c( "Ann Excess Return", "Ann Tracking Error",  "Max DD")
Rows_to_Perc <- Moments %>%
mutate(RN = row_number()) %>%
filter(!Info %in% c("Average Length")) %>%
pull(RN)
Cols_length <- ncol(Moments)
tab <- Moments %>%   gt::gt(caption = 'Attribution Overtime') %>%
tab_header(title = glue::glue("Cluster Performance")) %>%
fmt_percent(
columns = 3:Cols_length,
rows = Rows_to_Perc,
decimals = 1
) %>%
tab_style(
style = list(
cell_fill(color = 'gray27', alpha = 0.15),
cell_text(size = 'large', weight = 'bold',align = 'left')
),
locations = cells_row_groups())
tab <- tab %>%
tab_options(data_row.padding = px(4),table.width = pct(100),
column_labels.font.size = pct(50),
column_labels.vlines.width = 1, table.font.size = pct(80)) %>%
tab_options(data_row.padding = px(6),
column_labels.font.size = pct(100)) %>%
tab_style(style = cell_text(weight = 1200, align = 'left'),locations = cells_title(groups = 'title')) %>%
tab_style(style = cell_text(color = 'darkgrey', transform = 'uppercase', align = 'center'),
locations = cells_column_labels(everything()))
rolling_results[[i]] <- Moments
}
for (i in seq_along(rolling_results)) {
saveRDS(rolling_results[[i]], file = paste0("data/rolling_", i, ".rds"))
}
rolling_results
tab
# from the results, lets get some perfromance metrics
# clean our BM dataset
BM <- BM %>%
mutate(Year = format(date, "%Y"), Month = format(date, "%b"), YM = format(date, "%y %b")) %>%
filter(Month %in% c("Mar", "Jun", "Sep", "Dec")) %>%
select(date, BM)
rolling_results <- list()
for (i in seq_along(rolling_Backtest)) {
rolling_backtest <- rolling_Backtest[[i]]
df <- tbl2xts::tbl_xts(rolling_backtest, ret, cluster, Colnames_Exact = TRUE)
bm <- BM %>% filter(date >= ymd(20140331)) %>%  tbl2xts::tbl_xts(., Colnames_Exact = TRUE)
df[is.na(df)] <- 0
bm[is.na(bm)] <- 0
Moments <- bind_rows(data.frame(PerformanceAnalytics::Return.annualized.excess(df, bm, scale =4)) %>% round(., 3),
data.frame(PerformanceAnalytics::TrackingError(Ra = df, Rb = bm, scale =4 ))%>% round(., 3),
data.frame(PerformanceAnalytics::AdjustedSharpeRatio(df))%>% round(., 3),
data.frame(PerformanceAnalytics::AverageLength(df))%>% round(., 0),
data.frame( PerformanceAnalytics::maxDrawdown(df)%>% round(., 3))) %>%
tibble::rownames_to_column("Metrics") %>%
mutate("Investment Horizon" = glue::glue("{i} Year"), Info = c("Ann Excess Return", "Ann Tracking Error", "Adj. Sharpe Ratio", "DD Length", "Max DD")) %>%
relocate("Investment Horizon" , .before = Metrics) %>%  relocate("Info" , .before = Metrics) %>% as_tibble() %>% select(-Metrics)
Make_perc <-
c( "Ann Excess Return", "Ann Tracking Error",  "Max DD")
Rows_to_Perc <- Moments %>%
mutate(RN = row_number()) %>%
filter(!Info %in% c("Average Length")) %>%
pull(RN)
Cols_length <- ncol(Moments)
tab <- Moments %>%   gt::gt(caption = 'Attribution Overtime') %>%
tab_header(title = glue::glue("Cluster Performance")) %>%
fmt_percent(
columns = 3:Cols_length,
rows = Rows_to_Perc,
decimals = 1
) %>%
tab_style(
style = list(
cell_fill(color = 'gray27', alpha = 0.15),
cell_text(size = 'large', weight = 'bold',align = 'left')
),
locations = cells_row_groups())
tab <- tab %>%
tab_options(data_row.padding = px(4),table.width = pct(100),
column_labels.font.size = pct(50),
column_labels.vlines.width = 1, table.font.size = pct(80)) %>%
tab_options(data_row.padding = px(6),
column_labels.font.size = pct(100)) %>%
tab_style(style = cell_text(weight = 1200, align = 'left'),locations = cells_title(groups = 'title')) %>%
tab_style(style = cell_text(color = 'darkgrey', transform = 'uppercase', align = 'center'),
locations = cells_column_labels(everything()))
rolling_results[[i]] <- tab
}
for (i in seq_along(rolling_results)) {
saveRDS(rolling_results[[i]], file = paste0("data/rolling_", i, ".rds"))
}
for (i in seq_along(rolling_results)) {
saveRDS(rolling_results[[i]], file = paste0("data/rolling_", i, ".rds"))
}
# lets get rid of all the very volatile stock
volatile_stock <- cluster.merging %>%
mutate(Y = format(date, "%y") ) %>%
group_by(stock, Y) %>%
mutate(ret = px/lag(px)-1) %>%
summarize(date = last(date), SD = sd(ret, na.rm = T)*sqrt(12)) %>%
filter(SD>1) %>%
select(stock) %>%
distinct() %>%
pull()
# from this i can see that there are stock that have extreme volatility, lets filter out those that can compl
# from the results that we got from our initial cluster, let do some analysis
stock_analysis_data <- cluster.merging %>% filter(!stock %in% volatile_stock) %>%
select(date, cluster, stock) %>%
merge(., JSE_data %>% rename(stock = ticks), c("stock", "date")) %>%
filter(!is.na(cluster)) %>%
select(-Company, -vol) %>%
mutate(stock = str_remove(stock, ".JO"))
# lets find out returns and volatility by cluster and sector
Rolling_return_split <- stock_analysis_data %>%
group_by(stock) %>%
mutate(ret = px/lag(px)-1) %>%
mutate(RollRets = RcppRoll::roll_prod(1 + ret, 12, fill = NA,
align = "right")^(12/12) - 1) %>%
group_by(date) %>%
filter(any(!is.na(RollRets))) %>%
ungroup()
# write a loop function that gets the data and plots it
plot_return_list <-  list()
#
Rolling_return_list <- Rolling_return_split %>%group_by(cluster) %>%  group_split()
for (i in seq_along(Rolling_return_list)) {
df <- Rolling_return_list[[i]] %>% as.tibble()
g <- df %>%
ggplot() +
geom_line(aes(date, RollRets, color = stock), alpha = 0.7,
size = 1.25) +
labs(title = paste("Performance of Cluster Constituents", i),
subtitle = "", x = "", y = "Rolling 1 year Returns (Ann.)",
caption = "Note:\n Return series is similar") + theme_minimal()
plot_return_list[[i]] <- g
}
# create a loop to save
for (i in seq_along(plot_return_list)) {
png(paste0("data/returnplots_", i, ".png"), width = 400, height = 250)
plot(plot_return_list[[i]])
dev.off()
}
plot_volatility_list <-  list()
Rolling_sd_split <- stock_analysis_data %>%
group_by(stock) %>%
mutate(ret = px/lag(px)-1) %>%
mutate(RollSD = RcppRoll::roll_sd(1 + ret, 12, fill = NA, align = "right") *
sqrt(12)) %>%
filter(!is.na(RollSD))
# write a loop function that gets the data and plots it
plot_sd_list <-  list()
#
Rolling_sd_list <- Rolling_sd_split %>%group_by(cluster) %>%  group_split()
plot_sd_list <- list()  # Initialize an empty list to store the plots
for (i in seq_along(Rolling_sd_list)) {
df <- Rolling_sd_list[[i]] %>% as.tibble()
g <- df %>%
ggplot() +
geom_line(aes(date, RollSD, color = stock), alpha = 0.7, size = 1.25) +
labs(
title = paste("Volatility of Cluster Constituents", i),
subtitle = "",
x = "",
y = "Rolling 1 year standard deviation (Ann.)",
caption = "Vol series is similar"
) + theme_minimal()
plot_sd_list[[paste("Cluster", i)]] <- g
}
for (i in seq_along(plot_sd_list)) {
png(paste0("data/volplots_", i, ".png"), width = 400, height = 250)
plot(plot_sd_list[[i]])
dev.off()
}
plot_sd_list
plot_return_list
# lets get rid of all the very volatile stock
volatile_stock <- cluster.merging %>%
mutate(Y = format(date, "%y") ) %>%
group_by(stock, Y) %>%
mutate(ret = px/lag(px)-1) %>%
summarize(date = last(date), SD = sd(ret, na.rm = T)*sqrt(12)) %>%
filter(SD>1) %>%
select(stock) %>%
distinct() %>%
pull()
# from this i can see that there are stock that have extreme volatility, lets filter out those that can compl
# from the results that we got from our initial cluster, let do some analysis
stock_analysis_data <- cluster.merging %>% filter(!stock %in% volatile_stock) %>%
select(date, cluster, stock) %>%
merge(., JSE_data %>% rename(stock = ticks), c("stock", "date")) %>%
filter(!is.na(cluster)) %>%
select(-Company, -vol) %>%
mutate(stock = str_remove(stock, ".JO"))
# lets find out returns and volatility by cluster and sector
Rolling_return_split <- stock_analysis_data %>%
group_by(stock) %>%
mutate(ret = px/lag(px)-1) %>%
mutate(RollRets = RcppRoll::roll_prod(1 + ret, 12, fill = NA,
align = "right")^(12/12) - 1) %>%
group_by(date) %>%
filter(any(!is.na(RollRets))) %>%
ungroup()
# write a loop function that gets the data and plots it
plot_return_list <-  list()
#
Rolling_return_list <- Rolling_return_split %>%group_by(cluster) %>%  group_split()
for (i in seq_along(Rolling_return_list)) {
df <- Rolling_return_list[[i]] %>% as.tibble()
g <- df %>%
ggplot() +
geom_line(aes(date, RollRets, color = stock), alpha = 0.7,
size = 1.25) +
labs(title = paste("Performance of Cluster Constituents", i),
subtitle = "", x = "", y = "Rolling 1 year Returns (Ann.)",
caption = "Source:\n Yahoo Finance, Authors own calculations ") + theme_minimal()
plot_return_list[[i]] <- g
}
# create a loop to save
for (i in seq_along(plot_return_list)) {
png(paste0("data/returnplots_", i, ".png"), width = 400, height = 250)
plot(plot_return_list[[i]])
dev.off()
}
plot_volatility_list <-  list()
Rolling_sd_split <- stock_analysis_data %>%
group_by(stock) %>%
mutate(ret = px/lag(px)-1) %>%
mutate(RollSD = RcppRoll::roll_sd(1 + ret, 12, fill = NA, align = "right") *
sqrt(12)) %>%
filter(!is.na(RollSD))
# write a loop function that gets the data and plots it
plot_sd_list <-  list()
#
Rolling_sd_list <- Rolling_sd_split %>%group_by(cluster) %>%  group_split()
plot_sd_list <- list()  # Initialize an empty list to store the plots
for (i in seq_along(Rolling_sd_list)) {
df <- Rolling_sd_list[[i]] %>% as.tibble()
g <- df %>%
ggplot() +
geom_line(aes(date, RollSD, color = stock), alpha = 0.7, size = 1.25) +
labs(
title = paste("Volatility of Cluster Constituents", i),
subtitle = "",
x = "",
y = "Rolling 1 year standard deviation (Ann.)",
caption = "Yahoo Finance, Authors own calculations"
) + theme_minimal()
plot_sd_list[[paste("Cluster", i)]] <- g
}
for (i in seq_along(plot_sd_list)) {
png(paste0("data/volplots_", i, ".png"), width = 400, height = 250)
plot(plot_sd_list[[i]])
dev.off()
}
plot_sd_list
plot_return_list
rm(list = ls()) # Clean your environment:
gc() # garbage collection - It can be useful to call gc after a large object has been removed, as this may prompt R to return memory to the operating system.
require("pacman")
p_load("tidyquant", "fmxdat", "tidyverse", "PerformanceAnalytics", "lubridate", "DEoptim", "data.table", "covFcatorModel", "gt", "factoextra", "foreach")
list.files('code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
# load the daily stock data from the JSE, via yahoo finance
data  <- read_csv("data/JSE full list.csv")
# load the daily stock data from the JSE, via yahoo finance
data  <- read_csv("data/JSE full list .csv.csv")
# load the daily stock data from the JSE, via yahoo finance
data  <- read_csv("data/JSE full list.csv")
# load the daily stock data from the JSE, via yahoo finance
data  <- read_csv("data/JSE full list .csv")
# ranking.df <- read_csv("data/ranking_date.csv")
# I created the list of companies corresponding to sectors myself
stks  <- read_csv("data/JSE full list .csv") %>% select(Ticker) %>% rename(tcks = Ticker) %>%
mutate(tcks = str_remove(tcks, "JSE:"),
tcks = paste0(tcks, ".JO")) %>% select(tcks) %>% distinct() %>% pull()
#  # for the sector analysis
cater <- data %>% rename(ticks = Ticker) %>%
mutate(ticks = str_remove(ticks, "JSE:"),
ticks = paste0(ticks, ".JO"))
# #
#    # environment to store data
e <- new.env()
# #
#  # get symbols from YF#
getSymbols(stks, from="2014-01-01", env = e, show_col_types = FALSE)
rm(list = ls()) # Clean your environment:
gc() # garbage collection - It can be useful to call gc after a large object has been removed, as this may prompt R to return memory to the operating system.
require("pacman")
p_load("tidyquant", "fmxdat", "tidyverse", "PerformanceAnalytics", "lubridate", "DEoptim", "data.table", "covFcatorModel", "gt", "factoextra", "foreach")
list.files('code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
# load the daily stock data from the JSE, via yahoo finance
data  <- read_csv("data/JSE full list .csv")
# ranking.df <- read_csv("data/ranking_date.csv")
# I created the list of companies corresponding to sectors myself
stks  <- read_csv("data/JSE full list .csv") %>% select(Ticker) %>% rename(tcks = Ticker) %>%
mutate(tcks = str_remove(tcks, "JSE:"),
tcks = paste0(tcks, ".JO")) %>% select(tcks) %>% distinct() %>% pull()
#  # for the sector analysis
cater <- data %>% rename(ticks = Ticker) %>%
mutate(ticks = str_remove(ticks, "JSE:"),
ticks = paste0(ticks, ".JO"))
# #
#    # environment to store data
e <- new.env()
# #
#  # get symbols from YF#
getSymbols(stks, from="2014-01-01", env = e, show_col_types = FALSE)
